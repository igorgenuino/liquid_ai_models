# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10eA2JoWCrr-T26dRB7D8zXKMsTTnIeJM
"""

# Use -U to ensure you get recent versions of the libraries
!pip install -q -U transformers datasets accelerate peft trl bitsandbytes

import torch
import transformers
import trl
import os
from datasets import load_dataset
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForImageTextToText, AutoProcessor, TrainingArguments, Trainer
import math

# Disable Weights and Biases logging
os.environ["WANDB_DISABLED"] = "true"

print(f"üì¶ PyTorch version: {torch.__version__}")
print(f"ü§ó Transformers version: {transformers.__version__}")
print(f"üìä TRL version: {trl.__version__}")

model_id = "LiquidAI/LFM2-VL-450M"

print("üìö Loading processor...")
processor = AutoProcessor.from_pretrained(
    model_id,
    trust_remote_code=True,
)

print("üß† Loading model for GPU...")
model = AutoModelForImageTextToText.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16, # Use bfloat16 for faster training on modern GPUs
    trust_remote_code=True,
    device_map="auto", # Automatically use the available GPU
)

print("\n‚úÖ Model loaded successfully!")

print("\nüîÑ Loading and formatting dataset...")
raw_ds = load_dataset("akahana/Driver-Drowsiness-Dataset")

# --- Use a small subset for testing ---
print(f"‚ö†Ô∏è  Reducing dataset size for a quick test run.")
train_dataset_raw = raw_ds["train"].select(range(200)) # Use only 200 samples for training
eval_dataset_raw = raw_ds["test"].select(range(50))   # Use only 50 samples for evaluation
# --- END ---

system_message = (
    "You are a Vision Language Model specialized in analyzing face images and determining if a person is drowsy or not. "
    "Provide a concise answer based on the image and question."
)

# --- MODIFIED FORMATTING FUNCTION ---
def format_drowsiness_sample(sample):
    label_text = "Drowsy" if sample["label"] == 0 else "Non Drowsy"
    question_text = "Is the person in the image drowsy or not?"

    # --- THIS IS THE KEY CHANGE ---
    # The user's content now needs BOTH an image placeholder and the text.
    messages = [
        {"role": "system", "content": [{"type": "text", "text": system_message}]},
        {
            "role": "user",
            "content": [
                {"type": "image"}, # This placeholder adds the <image> token to the text
                {"type": "text", "text": question_text}
            ],
        },
        {"role": "assistant", "content": [{"type": "text", "text": label_text}]},
    ]
    # --- END OF KEY CHANGE ---

    # The image is still returned separately in a list.
    return {"messages": messages, "images": [sample["image"]]}


# Apply the new formatting function
features_to_remove = list(train_dataset_raw.features)
if "image" in features_to_remove:
    features_to_remove.remove("image")

train_dataset = train_dataset_raw.map(format_drowsiness_sample, remove_columns=features_to_remove)
eval_dataset = eval_dataset_raw.map(format_drowsiness_sample, remove_columns=features_to_remove)


print(f"‚úÖ SFT Dataset formatted. Train samples: {len(train_dataset)}, Eval samples: {len(eval_dataset)}")
print(f"Dataset columns: {train_dataset.column_names}")
print("\nExample of a formatted sample:")
print(train_dataset[0])

target_modules = [
    "q_proj", "v_proj", "fc1", "fc2", "linear",
    "gate_proj", "up_proj", "down_proj",
]

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.05,
    r=8,
    bias="none",
    target_modules=target_modules,
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# --- ADD THIS LINE ---
# Attach the tokenizer to the model object
model.tokenizer = processor.tokenizer

from trl import SFTTrainer, SFTConfig

# Configure the training using SFTConfig
training_args = SFTConfig(
    output_dir="lfm2-drowsiness-finetuned-TEST",
    max_steps=20,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    warmup_ratio=0.1,
    lr_scheduler_type="cosine",
    bf16=True,
    do_eval=True,
    eval_steps=10,
    save_steps=10,
    logging_steps=5,
    dataset_kwargs={"skip_prepare_dataset": True},
    report_to="none",
)


# Create the SFTTrainer
# The 'tokenizer' and 'dataset_text_field' arguments are removed.
# The trainer will find the tokenizer on the model and the 'text' column in the dataset by default.
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

print("\nüöÄ Starting GPU training with SFTTrainer...")
trainer.train()

print("\nüéâ Training completed!")
trainer.save_model()
print(f"üíæ Model saved to: {training_args.output_dir}")